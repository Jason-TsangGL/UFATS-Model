{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shuffle\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np,sys,os\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.ndimage import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(one, two):\n",
    "    return os.path.join(one, two)\n",
    "\n",
    "def check_path(path):\n",
    "    count=0\n",
    "    for files in sorted(os.listdir(path)):\n",
    "        image_path = os.path.join(path,files)\n",
    "        count+=1\n",
    "    return count\n",
    "\n",
    "num_patients =check_path(join(os.getcwd(), \"dataset\"))\n",
    "folder_pat h=join(os.getcwd(), \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(one, two):\n",
    "    return os.path.join(one, two)\n",
    "\n",
    "def check_path(path):\n",
    "    count=0\n",
    "    for files in sorted(os.listdir(path)):\n",
    "        image_path = os.path.join(path,files)\n",
    "        count+=1\n",
    "    return count\n",
    "\n",
    "\n",
    "InputImage\n",
    "for patient in sorted(os.listdir(folder_path)):\n",
    "    patient_path = join(folder_path, patient)\n",
    "    InputImage=sitk.ReadImage(sitk.ImageSeriesReader_GetGDCMSeriesFileNames(patient_path)) \n",
    "inputs = np.empty((num_patients * InputImage.GetSize()[2], int(InputImage.GetSize()[0]/2), int(InputImage.GetSize()[1]), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcm_to_np(path):\n",
    "    global dic_count\n",
    "    try:\n",
    "        slice_filenames = sitk.ImageSeriesReader_GetGDCMSeriesFileNames(path)\n",
    "        image = sitk.ReadImage(slice_filenames)\n",
    "        for x in range(15):\n",
    "            current_slice = image[:, :, x]\n",
    "            current_array = sitk.GetArrayFromImage(current_slice)\n",
    "            split_array = current_array[:,:-256]\n",
    "            inputs[dic_count] = split_array[:, :, np.newaxis]\n",
    "            dic_count+=1\n",
    "        print(\"Importing...\")\n",
    "        print(f'{patient} {patient_count}/{num_patients}')\n",
    "        print(\"_____________________________\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fleoutputs=np.empty((num_patients * InputImage.GetSize()[2], int(InputImage.GetSize()[0]/2), int(InputImage.GetSize()[1]), 1))\n",
    "fle_count = 0\n",
    "dic_count = 0\n",
    "ann_count = 0\n",
    "patient_count=0\n",
    "try:\n",
    "    for patient in sorted(os.listdir(folder_path)):\n",
    "        patient_count+=1\n",
    "        clear_output(wait=True)\n",
    "        patient_path = join(folder_path, patient)\n",
    "        dcm_to_np(patient_path)\n",
    "        fle_folder = join(patient_path, \"FLE\")\n",
    "        for fle in os.listdir(fle_folder):\n",
    "            if(\".nrrd\" in fle):\n",
    "                flesegmentation = sitk.ReadImage(join(fle_folder, fle))\n",
    "                fle_to_np(flesegmentation,fleoutputs)\n",
    "    clear_output(wait=True)\n",
    "    print(\"Import Complete.\")\n",
    "    print(f'{patient_count} Patient DCM(s) found.')\n",
    "    print('MinMaxScalar Operator...')\n",
    "    inputs = MinMaxScaler(feature_range=(0, 1.0)).fit_transform(inputs.reshape((length, 256*256))).reshape((length, 256, 256, 1))\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for inhomogeneity correction \n",
    "def correct_roi(image):\n",
    "    inputImage=sitk.GetImageFromArray(image)\n",
    "    inputImage = sitk.Cast(inputImage, sitk.sitkFloat32 )\n",
    "    corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
    "    output = corrector.Execute( inputImage)\n",
    "    image_c= sitk.GetArrayFromImage(output)\n",
    "    image_c=cv2.normalize(src=image_c, dst=None, alpha=0.0, beta=255.0, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U) #need to normalize, not direct conversion by \"np.uint8\"\n",
    "    return image_c\n",
    "\n",
    "def IHcorrection (inputimage):\n",
    "    try:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Normalizing...\")\n",
    "        print(f'{x+1}/{totalslices} Slices |{round((x/totalslices)*100)}%|')\n",
    "        temp = inputimage.reshape(256, 256)\n",
    "        temp1= correct_roi(temp)\n",
    "        np.linalg.norm(temp1)\n",
    "        inputslice=temp1[:, :, np.newaxis]\n",
    "        if x == totalslices:\n",
    "            print('complete.')\n",
    "    except Exception as e:\n",
    "        print(f'Inhomogeneity Correction Error Slice {x+1}, {e}')\n",
    "    return inputslice\n",
    "\n",
    "ImportedArray = np.empty((num_patients * InputImage.GetSize()[2], int(InputImage.GetSize()[0]/2), int(InputImage.GetSize()[1]), 9))\n",
    "for x in range (totalslices):\n",
    "    ImportedArray[x] = IHcorrection (inputs[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hw66ln2H3YzQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(678)\n",
    "tf.set_random_seed(5678)\n",
    "\n",
    "def tf_relu(x): return tf.nn.relu(x)\n",
    "def d_tf_relu(s): return tf.cast(tf.greater(s,0),dtype=tf.float32)\n",
    "def tf_softmax(x): return tf.nn.softmax(x)\n",
    "def np_sigmoid(x): 1/(1 + np.exp(-1 *x))\n",
    "\n",
    "# --- make class ---\n",
    "class conlayer_left():\n",
    "    \n",
    "    def __init__(self,ker,in_c,out_c):\n",
    "        self.w = tf.Variable(tf.random_normal([ker,ker,in_c,out_c],stddev=0.05))\n",
    "\n",
    "    def feedforward(self,input,stride=1,dilate=1):\n",
    "        self.input  = input\n",
    "        self.layer  = tf.nn.conv2d(input,self.w,strides = [1,stride,stride,1],padding='SAME')\n",
    "        self.layerA = tf_relu(self.layer)\n",
    "        return self.layerA\n",
    "\n",
    "class conlayer_right():\n",
    "    \n",
    "    def __init__(self,ker,in_c,out_c):\n",
    "        self.w = tf.Variable(tf.random_normal([ker,ker,in_c,out_c],stddev=0.05))\n",
    "\n",
    "    def feedforward(self,input,stride=1,dilate=1,output=1):\n",
    "        self.input  = input\n",
    "\n",
    "        current_shape_size = input.shape\n",
    "\n",
    "        self.layer = tf.nn.conv2d_transpose(input,self.w,\n",
    "        output_shape=[batch_size] + [int(current_shape_size[1].value*2),int(current_shape_size[2].value*2),int(current_shape_size[3].value/2)],strides=[1,2,2,1],padding='SAME')\n",
    "        self.layerA = tf_relu(self.layer)\n",
    "        return self.layerA\n",
    "\n",
    "# # --- get data ---\n",
    "# data_location = \"./dataset/\"\n",
    "# train_data = []  # create an empty list\n",
    "# for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
    "#     for filename in fileList:\n",
    "#         if \".tif\" in filename.lower():  # check whether the file's DICOM\n",
    "#             train_data.append(os.path.join(dirName,filename))\n",
    "\n",
    "# data_location = \"./DRIVE/training/1st_manual/\"\n",
    "# train_data_gt = []  # create an empty list\n",
    "# for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
    "#     for filename in fileList:\n",
    "#         if \".tif\" in filename.lower():  # check whether the file's DICOM\n",
    "#             train_data_gt.append(os.path.join(dirName,filename))\n",
    "\n",
    "\n",
    "# train_images = np.zeros(shape=(128,256,256,1))\n",
    "# train_labels = np.zeros(shape=(128,256,256,1))\n",
    "\n",
    "# for file_index in range(len(train_data)):\n",
    "#     train_images[file_index,:,:]   = np.expand_dims(imresize(imread(train_data[file_index],mode='F',flatten=True),(256,256)),axis=2)\n",
    "#     train_labels[file_index,:,:]   = np.expand_dims(imresize(imread(train_data_gt[file_index],mode='F',flatten=True),(256,256)),axis=2)\n",
    "\n",
    "train_images = ImportedArray\n",
    "train_labels = fleoutputs\n",
    "\n",
    "train_images = (train_images - train_images.min()) / (train_images.max() - train_images.min())\n",
    "train_labels = (train_labels - train_labels.min()) / (train_labels.max() - train_labels.min())\n",
    "\n",
    "# --- hyper ---\n",
    "num_epoch = 100\n",
    "init_lr = 0.0001\n",
    "batch_size = 2\n",
    "\n",
    "# --- make layer ---\n",
    "# left\n",
    "l1_1 = conlayer_left(3,1,3)\n",
    "l1_2 = conlayer_left(3,3,3)\n",
    "l1_3 = conlayer_left(3,3,3)\n",
    "\n",
    "l2_1 = conlayer_left(3,3,6)\n",
    "l2_2 = conlayer_left(3,6,6)\n",
    "l2_3 = conlayer_left(3,6,6)\n",
    "\n",
    "l3_1 = conlayer_left(3,6,12)\n",
    "l3_2 = conlayer_left(3,12,12)\n",
    "l3_3 = conlayer_left(3,12,12)\n",
    "\n",
    "l4_1 = conlayer_left(3,12,24)\n",
    "l4_2 = conlayer_left(3,24,24)\n",
    "l4_3 = conlayer_left(3,24,24)\n",
    "\n",
    "l5_1 = conlayer_left(3,24,48)\n",
    "l5_2 = conlayer_left(3,48,48)\n",
    "l5_3 = conlayer_left(3,48,24)\n",
    "\n",
    "# right\n",
    "l6_1 = conlayer_right(3,24,48)\n",
    "l6_2 = conlayer_left(3,24,24)\n",
    "l6_3 = conlayer_left(3,24,12)\n",
    "\n",
    "l7_1 = conlayer_right(3,12,24)\n",
    "l7_2 = conlayer_left(3,12,12)\n",
    "l7_3 = conlayer_left(3,12,6)\n",
    "\n",
    "l8_1 = conlayer_right(3,6,12)\n",
    "l8_2 = conlayer_left(3,6,6)\n",
    "l8_3 = conlayer_left(3,6,3)\n",
    "\n",
    "l9_1 = conlayer_right(3,3,6)\n",
    "l9_2 = conlayer_left(3,3,3)\n",
    "l9_3 = conlayer_left(3,3,3)\n",
    "\n",
    "l10_final = conlayer_left(3,3,1)\n",
    "\n",
    "# ---- make graph ----\n",
    "x = tf.placeholder(shape=[None,256,256,1],dtype=tf.float32)\n",
    "y = tf.placeholder(shape=[None,256,256,1],dtype=tf.float32)\n",
    "\n",
    "layer1_1 = l1_1.feedforward(x)\n",
    "layer1_2 = l1_2.feedforward(layer1_1)\n",
    "layer1_3 = l1_3.feedforward(layer1_2)\n",
    "\n",
    "layer2_Input = tf.nn.max_pool(layer1_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "layer2_1 = l2_1.feedforward(layer2_Input)\n",
    "layer2_2 = l2_2.feedforward(layer2_1)\n",
    "layer2_3 = l2_3.feedforward(layer2_2)\n",
    "\n",
    "layer3_Input = tf.nn.max_pool(layer2_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "layer3_1 = l3_1.feedforward(layer3_Input)\n",
    "layer3_2 = l3_2.feedforward(layer3_1)\n",
    "layer3_3 = l3_3.feedforward(layer3_2)\n",
    "\n",
    "layer4_Input = tf.nn.max_pool(layer3_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "layer4_1 = l4_1.feedforward(layer4_Input)\n",
    "layer4_2 = l4_2.feedforward(layer4_1)\n",
    "layer4_3 = l4_3.feedforward(layer4_2)\n",
    "\n",
    "layer5_Input = tf.nn.max_pool(layer4_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "layer5_1 = l5_1.feedforward(layer5_Input)\n",
    "layer5_2 = l5_2.feedforward(layer5_1)\n",
    "layer5_3 = l5_3.feedforward(layer5_2)\n",
    "\n",
    "layer6_Input = tf.concat([layer5_3,layer5_Input],axis=3)\n",
    "layer6_1 = l6_1.feedforward(layer6_Input)\n",
    "layer6_2 = l6_2.feedforward(layer6_1)\n",
    "layer6_3 = l6_3.feedforward(layer6_2)\n",
    "\n",
    "layer7_Input = tf.concat([layer6_3,layer4_Input],axis=3)\n",
    "layer7_1 = l7_1.feedforward(layer7_Input)\n",
    "layer7_2 = l7_2.feedforward(layer7_1)\n",
    "layer7_3 = l7_3.feedforward(layer7_2)\n",
    "\n",
    "layer8_Input = tf.concat([layer7_3,layer3_Input],axis=3)\n",
    "layer8_1 = l8_1.feedforward(layer8_Input)\n",
    "layer8_2 = l8_2.feedforward(layer8_1)\n",
    "layer8_3 = l8_3.feedforward(layer8_2)\n",
    "\n",
    "layer9_Input = tf.concat([layer8_3,layer2_Input],axis=3)\n",
    "layer9_1 = l9_1.feedforward(layer9_Input)\n",
    "layer9_2 = l9_2.feedforward(layer9_1)\n",
    "layer9_3 = l9_3.feedforward(layer9_2)\n",
    "\n",
    "layer10 = l10_final.feedforward(layer9_3)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(layer10-y))\n",
    "auto_train = tf.train.AdamOptimizer(learning_rate=init_lr).minimize(cost)\n",
    "\n",
    "# --- start session ---\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for iter in range(num_epoch):\n",
    "        \n",
    "        # train\n",
    "        for current_batch_index in range(0,len(train_images),batch_size):\n",
    "            current_batch = train_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
    "            current_label = train_labels[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
    "            sess_results = sess.run([cost,auto_train],feed_dict={x:current_batch,y:current_label})\n",
    "            print(' Iter: ', iter, \" Cost:  %.32f\"% sess_results[0],end='\\r')\n",
    "        print('\\n-----------------------')\n",
    "        train_images,train_labels = shuffle(train_images,train_labels)\n",
    "\n",
    "        if iter % 2 == 0:\n",
    "            test_example =   train_images[:2,:,:,:]\n",
    "            test_example_gt = train_labels[:2,:,:,:]\n",
    "            sess_results = sess.run([layer10],feed_dict={x:test_example})\n",
    "\n",
    "            sess_results = sess_results[0][0,:,:,:]\n",
    "            test_example = test_example[0,:,:,:]\n",
    "            test_example_gt = test_example_gt[0,:,:,:]\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(np.squeeze(test_example),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.title('Original Image')\n",
    "            plt.savefig('train_change/'+str(iter)+\"a_Original_Image.png\")\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(np.squeeze(test_example_gt),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.title('Ground Truth Mask')\n",
    "            plt.savefig('train_change/'+str(iter)+\"b_Original_Mask.png\")\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(np.squeeze(sess_results),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.title('Generated Mask')\n",
    "            plt.savefig('train_change/'+str(iter)+\"c_Generated_Mask.png\")\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(np.multiply(np.squeeze(test_example),np.squeeze(test_example_gt)),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.title(\"Ground Truth Overlay\")\n",
    "            plt.savefig('train_change/'+str(iter)+\"d_Original_Image_Overlay.png\")\n",
    "\n",
    "            plt.figure()\n",
    "            plt.axis('off')\n",
    "            plt.imshow(np.multiply(np.squeeze(test_example),np.squeeze(sess_results)),cmap='gray')\n",
    "            plt.title(\"Generated Overlay\")\n",
    "            plt.savefig('train_change/'+str(iter)+\"e_Generated_Image_Overlay.png\")\n",
    "\n",
    "            plt.close('all')\n",
    "\n",
    "    for data_index in range(0,len(train_images),batch_size):\n",
    "        current_batch = train_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
    "        current_label = train_labels[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
    "        sess_results = sess.run(layer10,feed_dict={x:current_batch})\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(np.squeeze(current_batch[0,:,:,:]),cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(str(data_index)+\"a_Original Image\")\n",
    "        plt.savefig('gif/'+str(data_index)+\"a_Original_Image.png\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(np.squeeze(current_label[0,:,:,:]),cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(str(data_index)+\"b_Original Mask\")\n",
    "        plt.savefig('gif/'+str(data_index)+\"b_Original_Mask.png\")\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(np.squeeze(sess_results[0,:,:,:]),cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(str(data_index)+\"c_Generated Mask\")\n",
    "        plt.savefig('gif/'+str(data_index)+\"c_Generated_Mask.png\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(np.multiply(np.squeeze(current_batch[0,:,:,:]),np.squeeze(current_label[0,:,:,:])),cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(str(data_index)+\"d_Original Image Overlay\")\n",
    "        plt.savefig('gif/'+str(data_index)+\"d_Original_Image_Overlay.png\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(np.multiply(np.squeeze(current_batch[0,:,:,:]),np.squeeze(sess_results[0,:,:,:])),cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(str(data_index)+\"e_Generated Image Overlay\")\n",
    "        plt.savefig('gif/'+str(data_index)+\"e_Generated_Image_Overlay.png\")\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "# -- end code --"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
